# 卸载ZFS存储池相关的硬盘尽量要先导出存储池（`zpool export file`）！！！
# 并禁用zfs-import@存储池.service自启（`systemctl disable zfs-import@file.service`）！！！
# LXC容器还原后编辑`nano /etc/pve/lxc/200.conf`，将里面的`rootfs: PRO:subvol-200-disk-1,size=2G`改为`rootfs: PRO:subvol-200-disk-0,size=2G`，从而使用已有的数据！！！

# 查看文件夹大小
```
du -h --max-depth=1
```

# 更换国内源

```
cp /etc/apt/sources.list /etc/apt/sources.list.bak && \
echo -e "\
# 清华大学源 \n\
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib \n\
deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib \n\
deb https://mirrors.tuna.tsinghua.edu.cn/debian-security bookworm-security main contrib \n\
" > /etc/apt/sources.list
```

# 临时禁用ipv6
```
echo 1 > /proc/sys/net/ipv6/conf/all/disable_ipv6
```
# 启用ipv6
```
echo 0 > /proc/sys/net/ipv6/conf/all/disable_ipv6
```

# 安装pv
```
apt-get update && apt-get install pv -y
```

# 查看CPU温度
acpitz是通过ACPI（高级配置和电源接口）提供的热区，通常由主板上的传感器测量，这些传感器位于CPU插槽附近。因此，acpitz通常反映的是CPU插座附近的温度，而不是CPU内部的温度。在某些情况下，acpitz可能显示的温度略低于实际的CPU温度，因为它测量的是CPU插座附近的温度，而不是CPU内部的温度
x86_pkg_temp是根据x86架构规范导出的温度数据，直接来自CPU内部。因此，它更准确地反映了CPU核心的实际温度
```
cat /sys/class/thermal/thermal_zone1/type \
&& echo $(($(cat /sys/class/thermal/thermal_zone1/temp) / 1000))°C
```

```
cat /sys/class/thermal/thermal_zone0/type \
&& echo $(($(cat /sys/class/thermal/thermal_zone0/temp) / 1000))°C
```

# 查看硬盘温度
```
smartctl -a /dev/sda | grep 'Temperature_Celsius' | awk '{print $10}'\
&& smartctl -a /dev/sdb | grep 'Temperature_Celsius' | awk '{print $10}'\
&& smartctl -a /dev/sdc | grep 'Temperature_Celsius' | awk '{print $10}'\
&& smartctl -a /dev/sdd | grep 'Temperature_Celsius' | awk '{print $10}'
```

# 接口添加主板温度、cpu温度
```
nano /usr/share/perl5/PVE/API2/Nodes.pm


        # 这个位置插入主板温度与cpu温度-开始
        $res->{'boottemperature'} = sprintf("%.2f", (join '', split /\n/, `cat /sys/class/thermal/thermal_zone0/temp`)/1000);
        $res->{'cputemperature'} = sprintf("%.2f", (join '', split /\n/, `cat /sys/class/thermal/thermal_zone1/temp`)/1000);
        # 这个位置插入主板温度与cpu温度-结束
        $res->{cpuinfo} = PVE::ProcFSTools::read_cpuinfo();
```

```
cp /file/DEV/docker_volumes/pve/Nodes.pm /usr/share/perl5/PVE/API2/Nodes.pm
```

# web页面显示温度 版本8.3.0
## 编辑/usr/share/pve-manager/js/pvemanagerlib.js
定位编辑位置，查找内容：Proxmox.Utils.render_cpu_model,
```
cp /file/DEV/docker_volumes/pve/pvemanagerlib.js /usr/share/pve-manager/js/pvemanagerlib.js
```

```
	{
	    itemId: 'cpus',
	    colspan: 2,
	    printBar: false,
	    title: gettext('CPU(s)'),
	    textField: 'cpuinfo',
	    renderer: Proxmox.Utils.render_cpu_model,
	    value: '',
	},
	{
        itemId: 'cputemperature',
        colspan: 2,
        printBar: false,
        title: gettext('CPU温度'),  // WEB显示内容
        textField: 'cputemperature',
        renderer:function(value){
		        return value+"℃";  // 输出格式
        }
	},
	{
        itemId: 'boottemperature',
        colspan: 2,
        printBar: false,
        title: gettext('主板温度'),  // WEB显示内容
        textField: 'boottemperature',
        renderer:function(value){
		        return value+"℃";  // 输出格式
        }
	},
```

继续查找内容(这个测试好像无关)：Ext.create('Ext.window.Window', {
```
	var reportWindow = Ext.create('Ext.window.Window', {
	    title: gettext('System Report'),
	    width: 1024,
	    height: 600, //初始值600
	    layout: 'fit',
	    modal: true,
```

继续查找内容跳转（每多一行数据增加 20）：widget.pveNodeStatus
```
Ext.define('PVE.node.StatusView', {
    extend: 'Proxmox.panel.StatusView',
    alias: 'widget.pveNodeStatus',

    height: 390, //初始值350
    bodyPadding: '15 5 15 5',
```


# 查看网卡配置
```shell
root@pve:~#  cat /etc/network/interfaces
# network interface settings; autogenerated
# Please do NOT modify this file directly, unless you know what
# you're doing.
#
# If you want to manage parts of the network configuration manually,
# please utilize the 'source' or 'source-directory' directives to do
# so.
# PVE will preserve these directives, but will NOT read its network
# configuration from sourced files, so do not attempt to move any of
# the PVE managed interfaces into external files!

auto lo
iface lo inet loopback

iface ens18 inet manual
#pve虚拟机使用

iface ens33 inet manual
#wmare虚拟机使用

iface enx50547ba01269 inet manual
#USB2.0百兆网卡白-摩外

iface enx00e04c680017 inet manual
#USB3.0千兆网卡

iface enx00e04c360424 inet manual
#USB2.0百兆网卡黑-MingLu

iface enp6s0 inet manual
#TAD1581板载网卡

iface enp7s0 inet manual
#TAD1581板载网卡 openwrt-LAN

iface enp8s0 inet manual
#TAD1581板载网卡 openwrt-WAN

iface enp1s0 inet manual
#j3455板载网卡

iface enp4s0 inet manual
#pcie2.5G网卡

iface enx461d616f5b15 inet manual
#xiaomi-note

auto vmbr0
iface vmbr0 inet static
        address 192.168.123.10/24
        bridge-ports enp1s0
        bridge-stp off
        bridge-fd 0

auto vmbr1
iface vmbr1 inet manual
        bridge-ports none
        bridge-stp off
        bridge-fd 0

auto vmbr10
iface vmbr10 inet manual
        bridge-ports enx461d616f5b15
        bridge-stp off
        bridge-fd 0

root@pve:~#
```

# 永久设置PVE-ZFS可使用内存为512M
```shell
echo -e "\
options zfs zfs_arc_min=$[1024*1024*1024-1] \n\
options zfs zfs_arc_max=$[1024*1024*1024] \n\
">/etc/modprobe.d/zfs.conf
```

# 开启直通配置
注：j3455-ZFS启动，开启pcie_acs_override=downstream,multifunction之后系统会死机，原因不详
## 开启iommu(EXT4安装的引导盘)
开启直通配置 开机引导添加intel_iommu=on iommu=pt pcie_acs_override=downstream,multifunction，Intel CPU为intel_iommu=on，AMD CPU为amd_iommu=on
```shell
nano /etc/default/grub

修改行GRUB_CMDLINE_LINUX_DEFAULT="quiet"
使其为GRUB_CMDLINE_LINUX_DEFAULT="quiet intel_iommu=on iommu=pt pcie_acs_override=downstream,multifunction"
```
### 更新引导
```shell
update-grub
```

## 开启iommu(ZFS安装的引导盘)
开启直通配置 开机引导添加intel_iommu=on iommu=pt pcie_acs_override=downstream,multifunction，Intel CPU为intel_iommu=on，AMD CPU为amd_iommu=on
```shell
cat /etc/kernel/cmdline

root=ZFS=rpool/ROOT/pve-1 boot=zfs rootdelay=10 quiet intel_iommu=on iommu=pt pcie_acs_override=downstream,multifunction
```

### 同步所有内核和初始化
```shell
proxmox-boot-tool refresh
```
## 验证其（开启iommu）有效性，从命令行运行“dmesg | grep -e DMAR -e IOMMU”。如果没有输出，则出现问题。极有可能是bios设置的问题，需要启动cpu的vt-x支持。
```shell
dmesg | grep -e DMAR -e IOMMU
```

## 加载vfio模块
```shell
cat /etc/modules

# 末尾新加
vfio
vfio_iommu_type1
vfio_pci
vfio_virqfd
```



# 更改任何相关模块后，您需要刷新 initramfs
```shell
update-initramfs -u -k all
```

# 导出ZFS存储池
```
zpool export backup
systemctl disable zfs-import@backup.service
```

# 导入ZFS存储池
```
zpool import file
```
## 设置开机自启
```
systemctl enable zfs-import@file.service
```
## 查看设置是否成功
```
systemctl status zfs-import@file.service
```
## 下面是一些其他辅助指令
```
zpool reguid file
```
```
cat /etc/zfs/zpool.cache
```
```
zpool set cachefile=none file
```


重新部署以前具有zpool的旧驱动器可能会在尝试将新池导入系统时导致名称冲突，除非先清除剩余的zpool元数据。

除非你有很好的备份，否则不要在你关心的数据驱动器上这样做，但这很容易修复：使用`zpool labelclear`擦除旧设备上的元数据。

cannot import 'backup': more than one matching pool

import by numeric ID instead

```
zpool labelclear -f /dev/disk/by-id/ata-ST2000DM001-1E6164_Z1E5QBVV
zpool labelclear -f /dev/disk/by-id/ata-WDC_WD20EZRZ-00Z5HB0_WD-WCC4N0FSD73A
```

# 数据中心--存储--添加目录
```
/file/PVE
```

# 关闭自动更新
```
systemctl disable pve-daily-update.timer
```


# 下面是ZFS相关优化


# 启用 SSD TRIM 机械硬盘忽略
SSD 需要时不时的 TRIM 来保证最佳性能与寿命。我们可以让 ZFS 在合适的时机自动执行 TRIM 操作
```
zpool set autotrim=on PRO
zpool get all PRO
```

## 手动触发
```
zpool trim PRO
```


# 禁用 atime (文件访问时间)
默认配置下，ZFS 会记录文件的最后访问时间。现实中，这个功能其实并不是很有用，但会导致大量写入并降低性能。如果你不需要知道文件访问时间，并不用任何依赖这个属性的程序的话（最常见的例子是一些邮件程序依赖 atime 来确定邮件是不是已读），我们可以关闭 atime 记录以降低磁盘写入压力。
```
zfs set atime=off PRO
zfs set atime=off PRO/DEV-docker
zfs set atime=off PRO/DEV-root
zfs set atime=off PRO/Gateway-docker
zfs set atime=off PRO/PRO-docker
zfs set atime=off PRO/Pvt-docker
zfs set atime=off PRO/subvol-200-disk-0
zfs set atime=off PRO/subvol-300-disk-0
zfs set atime=off PRO/subvol-400-disk-0
zfs set atime=off PRO/subvol-500-disk-0
zfs set atime=off backup
zfs set atime=off file
zfs set atime=off file/DEV
zfs set atime=off file/Gateway
zfs set atime=off file/PRO
zfs set atime=off file/PVE
zfs set atime=off file/Pvt
zfs set atime=off file/file
zfs set atime=off file/fuguang
zfs set atime=off rpool
zfs set atime=off rpool/ROOT
zfs set atime=off rpool/ROOT/pve-1
zfs set atime=off rpool/data
zfs set atime=off rpool/var-lib-vz
```


```
root@pve:~# cat /proc/spl/kstat/zfs/arcstats
c                               4    1073741824
c_min                           4    1073741823
c_max                           4    1073741824
size                            4    1063014024

# c is the target size of the ARC in bytes
# c是ARC的目标大小（以字节为单位）

# c_max is the maximum size of the ARC in bytes
# c_max是ARC的最大大小（以字节为单位）

# size is the current size of the ARC in bytes
# size是ARC的当前大小（以字节为单位）


root@pve:~# cat /sys/module/zfs/parameters/zfs_arc_max
1073741824

root@pve:~# cat /sys/module/zfs/parameters/zfs_arc_min
1073741823
```

